# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ILApKhNsw2Z6O-HhhLlp3fxn8riRenyG
"""

!pip install pandas
!pip install tensorflow

import os
import pandas as pd
import re
import tensorflow as tf

data_dir = 'drive/MyDrive/Data'
document = os.listdir(data_dir)

"""Me-list semua text file yang ada dalam folder terkait untuk proses loading ke dalam variabel"""

data_text = {'category':[],'line':[]}
line_loaded = 0
for i in document:
    print(f"PROCESSING {i}.....")
    document_data = open(os.path.join(data_dir,i), "r")
    data_list = []
    for line in document_data:
        if "#" in line:
            continue
        data_list.append(line)
        line_loaded += 1
    document_data.close()
    for line in data_list:
        data_text['category'].append(line[:4])
        data_text['line'].append(line[4:])
    print(f'{i} HAS BEEN LOADED\n')
print(f'\n{line_loaded} TEXT LINE HAS BEEN LOADED SUCCESSFULLY')
data_text_frame = pd.DataFrame.from_dict(data_text)

"""Proses loading semua texfile dengan pemisahan antara bagian kategori (category) dan deskripsi (line) kedalam dataframe."""

data_text_frame['line'] = data_text_frame['line'].apply(lambda x:" ".join(x.rstrip('\t\n').split()))
data_text_frame['category'] = data_text_frame['category'].apply(lambda x:" ".join(x.rstrip('\t\n').split()))
data_text_frame.head()

"""Cek dan modifikasi dataframe dengan beberapa fungsi separasi escape character untuk memastikan textfile telah termuat sesuai yang diharapkan"""

data_text_frame.info()

"""Cek jumlah data apakah ada kejanggalan seperti null value"""

data_text_frame['category'].value_counts()

"""Cek jumlah kategori dalam data teks. Terlihat ada ketimpangan jumlah yang tentunya memengaruhi proses learning model (bisa jadi model cenderung mengenali kategori terbanyak daripada yang lain)"""

from typing import List
def split_data(data:object,test_size:float=0.2)->object:
    len_data = len(data)
    test_portion = int(test_size*len_data)
    test_ind = len_data - test_portion
    train_data = data.iloc[:test_ind]
    test_data = data.iloc[test_ind:]
    return train_data, test_data
training_data,prediction_data = split_data(data_text_frame,0.1)

"""Memisahkan antara data training dan prediction. Data training akan dibagi lagi menjadi train_data dan test_data untuk proses learning/fitting. Pembagian ini tidak dilakukan shuffle dikarenakan data teks diinterpretasikan seabgai kesatuan yang memiliki arti semantik dari satu line ke yang lain."""

len(data_text_frame),len(training_data),len(prediction_data)

"""Melihat data mentah, subset training, dan subset prediksi."""

train_data,test_data = split_data(training_data,0.1)
train_data.head()

"""Pembagian subset training menjadi sub-subset training (train_data) dan sub-subset testing (test_data). Pembagian tidak dilakukan shuffling dengan alasan yang sama dan penentuan porsi yang kecil (0.1) dilihat dari proses learning yang mana test_data akan dijadikan validasi data. Model akan berupaya memperbaiki kualitas akurasi dengan mempertimbangkan hasil validasi juga."""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

vocab_size = 10000
embedding_dim = 16
max_length = 200
trunc_type='post'
padding_type = 'post'
oov_tok = "<OOV>"

training_sentences = train_data['line'].values
testing_sentences = test_data['line'].values
training_labels = train_data['category'].values
testing_labels = test_data['category'].values
all_labels = data_text_frame['category'].values

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(data_encoded['line'].values)
word_index = tokenizer.word_index
training_sequences = tokenizer.texts_to_sequences(training_sentences)
training_padded = pad_sequences(training_sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)

label_tokenizer = Tokenizer()
label_tokenizer.fit_on_texts(all_labels)
training_label_seq = np.asarray(label_tokenizer.texts_to_sequences(training_labels))
testing_label_seq = np.asarray(label_tokenizer.texts_to_sequences(testing_labels))

"""# Pre-processing textdata

## Parameter Definition
Menetapkan parameter vocab_size (kumpulan representasi numerik dari setiap kata dalam teksdata dengan jumlah tertentu), embedding_dim (dimensional mapping untuk setiap representasi numerik dari kata dalam teks data), max_length (panjang kata yang diizinkan untuk dimuat dalam text encoding-merubah kata ket dalam numerik melalui tokenizer, trunc_type (opsi arah pemotongan text data sehingga memenuhi max length), padding_type (opsi arah penambahan leading-zero agar encoding text memenuhi max length), oov_tok (representasi numerik untuk kata yang tidak sempat direpresentasikan karena telah memenuhi vocab_size).

## Tokenizing 
Mengubah text data ke dalam representasi numerik sesuai dengan vocabulary yang telah terbuat selama fase fitting. Beberapa modifikasi seperti padding dilakukan untuk memenuhi max_length untuk setiap data text. Konversi token untuk label dilakukan dengan mengubahnya menjadi array menyesuaikan konfigurasi tensorflow sequential model.
"""

training_label_seq

len(training_padded), len(training_sequences)

len(testing_padded), len(testing_sequences)

len(train_data),len(training_padded)

len(test_data),len(testing_padded)

len(label_tokenizer.word_index)

label_tokenizer.word_index

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),
    tf.keras.layers.Dense(embedding_dim, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(len(label_tokenizer.word_index)+1, activation='softmax')
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

"""Membangun model sekuensial dengan melibatkan Embedding layers (membuat dimensional mapping sehingga proses klasifikasi lebih robust), Bidirectional LSTM ( mempertimbangkan data sebelum dan sesudahnya dalam hal ini kata dalam data text), Dense (melakukan proses matematis dengan parameter aktivasi dan jumlah neuron), Dropout (melakukan deaktivasi layer untuk mengurangi potensi overfitting). Kompilasi dilakukan dengan memakai sparse entropy melihat output model berupa distribusi probabilitas (multiclass) target value dengan optimizer adam (kecenderungan robust yang tinggi) dan metrics accuracy (acuan penilaian/learning)."""

num_epochs = 50
history = model.fit(training_padded, training_label_seq, epochs=num_epochs, validation_data=(testing_padded, testing_label_seq))

"""Proses learning model sebanyak 50 iterasi disertai dengan validasi data memakai test_data untuk melihat sejauh mana model bisa memprediksi unseen data."""

import matplotlib.pyplot as plt


plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

"""Terlihat pola fluktuatif pada accuracy dan loss plot. Bisa jadi hal ini terjadi dikarenakan model mencapai local optima yang kemudian melakukan optimasi learning lagi. Pola subset train yang lebih baik (menurun untuk loss dan naik untuk akurasi) daripada subset test menunjukkan model tidak mengalami overfitting."""

prediction_data.head()

"""Melakukan prediksi dengan brand-new data (prediction subset) untuk mengetahui sejauh mana tingkat akurasi model."""

prediction_sentences = prediction_data['line'].values
prediction_sequences = tokenizer.texts_to_sequences(prediction_sentences)
prediction_padded    = pad_sequences(prediction_sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)
len(prediction_sequences),len(prediction_padded)

"""Konversi (tokenisasi) subset prediction dilakukan karena input data saat learning merupakan hasil tokenisasi (ditambah padding). Proses tokenisasi menggunakan tokenizer saat model lerning untuk mengurangi potensi data-leakage dari subset prediction (jika menggunakan tokenizer baru yang telah di-fitting dengan prediction, maka bisa jadi ada kemungkinan leakage atau error pada model dikarenakan representasi atau dimensional mapping yang berbeda)"""

prediction_result = model.predict(prediction_padded)

"""Hasil prediksi berupa sparse vector (List) yang menunjukkan tingkat probabilitas kategori (index mana yang tertinggi nilainya) untuk setiap data prediction. """

inverse = {code:name for name,code in label_tokenizer.word_index.items()}
prediction_cat = [inverse[elem.argmax()].upper() for elem in prediction_result]

"""Konversi dari sparse vector ke dalam kategori dengan inversi tokenizer."""

false_pred = 0
true_pred = 0
prediction_labels = prediction_data['category'].values
for i in range(len(prediction_cat)):
    if prediction_cat[i] == prediction_labels[i]:
        true_pred += 1
    else:
        false_pred += 1
print(f"True Prediction     : {true_pred}")
print(f"False Prediction    : {false_pred}")
print(f"Accuracy            : {true_pred/len(prediction_labels):.4f}")

prediction_label_seq = np.asarray(label_tokenizer.texts_to_sequences(prediction_labels))
model.evaluate(prediction_padded,prediction_label_seq)

"""Statistik prediksi data prediction oleh model"""

model.save('my_model.h5')

"""Menyimpan model untuk kebutuhan mendatang sehingga tidak perlu melakukan fitting model lagi atau untuk pembanding.

# ALTERNATIF KEDEPAN

- Melakukan hyperparamter tunning pada sekuensial model dan tokenizer.
- Menerapkan stopwords dari word_cortex seperti nltk, gloVe, spaCy, dsb
- Menerapkan mekanisme stopping dan graph untuk efisiensi pada proses fitting dan model saving.
- Memakai model pre-learning seperti BERT (Transfer Learning).
"""